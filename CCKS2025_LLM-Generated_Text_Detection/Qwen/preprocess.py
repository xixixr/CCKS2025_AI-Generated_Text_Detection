#%%
# 将训练集进行处理成指定格式符合大模型微调或者单纯在文本前加入prompt勇于回归
import datasets
import json
import os
from tqdm import tqdm

train_path = "/data/workspace/xiarui/tianchi_LMTextDetect/CCKS2025_LLM-Generated_Text_Detection/dataset/train.jsonl"

train_data = datasets.load_dataset("json", data_files=train_path, split="train",num_proc=8)


def make_prompt(text:str ="hhh") -> str:
    """ 将文本转换为模型的提示词格式
    Args:
        text (str): 输入的文本内容
    Returns:
        str: 格式化后的提示词
    """
    promt = " Analyze the following text and determine if it was generated by a human or AI."
    return promt

def map_function_prompt(example):
    """ 将每个样本转换为模型的输入格式
    Args:
        example (dict): 输入样本
    Returns:
        dict: 转换后的样本
    """
    text = example["text"]
    label = example["label"]
    prompt = make_prompt(text)
    
    return {
        "instruction": prompt,
        "input": text,
        "output": "AI" if label == 1 else "Human"
    }
    
def map_function(example):
    text = example["text"]
    example["text"] = make_prompt() + text
    return example
# 使用map函数处理数据集
train_data_processed = train_data.map(map_function, num_proc=8)
output_path = "/data/workspace/xiarui/tianchi_LMTextDetect/CCKS2025_LLM-Generated_Text_Detection/dataset/processed/train_prompt.jsonl"
# 保存处理后的数据集到指定路径
with open(output_path, "w") as f:
    for data in tqdm(train_data_processed):
        f.write(json.dumps(data, ensure_ascii=False) + "\n")



#%%
# 分离数据集
import datasets
import json
from tqdm import tqdm
data_path = "/data/workspace/xiarui/tianchi_LMTextDetect/CCKS2025_LLM-Generated_Text_Detection/dataset/train.jsonl"
data = datasets.load_dataset("json",data_files={"data":data_path},num_proc=4)["data"]
split_data = data.train_test_split(test_size=0.1,seed=114514)
train_data = split_data["train"]
val_data = split_data["test"]

train_data_path = "/data/workspace/xiarui/tianchi_LMTextDetect/CCKS2025_LLM-Generated_Text_Detection/dataset/processed/train.jsonl"
with open(train_data_path, "w") as f:
    for d in tqdm(train_data):
        f.write(json.dumps(d, ensure_ascii=False) + "\n")
        
val_data_path = "/data/workspace/xiarui/tianchi_LMTextDetect/CCKS2025_LLM-Generated_Text_Detection/dataset/processed/val.jsonl"
with open(val_data_path, "w") as f:
    for d in tqdm(val_data):
        f.write(json.dumps(d, ensure_ascii=False) + "\n")

# %%
